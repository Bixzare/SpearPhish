{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42524b2-6284-46e0-9fbd-86db1e733da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing function...\n",
      "\n",
      "Downloading required NLTK data...\n",
      "Loaded vectorizer from l1_vectorizer.pkl\n",
      "✓ Preprocessing successful!\n",
      "Cleaned text: account click _URL_ verify identity need password credit card information within _NUM_...\n",
      "Feature shape: (1, 5000)\n",
      "\n",
      "==================================================\n",
      "Testing complete prediction pipeline...\n",
      "\n",
      "Downloading required NLTK data...\n",
      "✓ Prediction: Phishing Email\n",
      "  Is Phishing: True\n",
      "  Confidence: 94.54%\n",
      "  Probabilities:\n",
      "    - Phishing Email: 94.54%\n",
      "    - Safe Email: 5.46%\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Cache for vectorizer (loaded once)\n",
    "_VECTORIZER_CACHE = None\n",
    "\n",
    "def preprocess_email_for_inference(email_text, vectorizer_path='l1_vectorizer.pkl'):\n",
    "    \"\"\"\n",
    "    Preprocess a single email text for phishing detection inference.\n",
    "    \n",
    "    Args:\n",
    "        email_text (str): Raw email text to preprocess\n",
    "        vectorizer_path (str): Path to the saved TF-IDF vectorizer\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (cleaned_text, vectorized_features)\n",
    "            - cleaned_text (str): Preprocessed text string\n",
    "            - vectorized_features (sparse matrix): TF-IDF features ready for model prediction\n",
    "            \n",
    "    Raises:\n",
    "        ValueError: If email_text is empty or not a string\n",
    "        FileNotFoundError: If vectorizer file is not found\n",
    "    \"\"\"\n",
    "    global _VECTORIZER_CACHE\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(email_text, str):\n",
    "        raise ValueError(f\"Input must be a string, got {type(email_text).__name__}\")\n",
    "    \n",
    "    if not email_text or email_text.strip() == \"\":\n",
    "        raise ValueError(\"Empty string - no content to process\")\n",
    "    \n",
    "    # Ensure NLTK data is available\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        print(\"Downloading required NLTK data...\")\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "    \n",
    "    # Load vectorizer (with caching)\n",
    "    if _VECTORIZER_CACHE is None:\n",
    "        try:\n",
    "            _VECTORIZER_CACHE = joblib.load(vectorizer_path)\n",
    "            print(f\"Loaded vectorizer from {vectorizer_path}\")\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Vectorizer not found at {vectorizer_path}. \"\n",
    "                \"Please ensure the model has been trained and saved.\"\n",
    "            )\n",
    "    \n",
    "    # Initialize preprocessing tools\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add email-specific junk words\n",
    "    email_junk = {\n",
    "        're', 'fw', 'fwd', 'subject', 'date', 'from', 'to', 'cc',\n",
    "        'spama', 'spamassassin', 'razor', 'exmh', 'rpm-l',\n",
    "        'nbsp', 'html', 'font', 'http', 'https'\n",
    "    }\n",
    "    stop_words.update(email_junk)\n",
    "    \n",
    "    # --- CLEANING PIPELINE ---\n",
    "    \n",
    "    # Step 1: Fix encoding & remove junk\n",
    "    text = email_text.replace(u'\\u00a0', ' ')  # Non-breaking space\n",
    "    text = re.sub(r'^\\s*>\\s?', '', text, flags=re.MULTILINE)  # Reply quotes\n",
    "    text = re.sub(r'[-_=]{4,}', '', text)  # Separators\n",
    "    \n",
    "    # Step 2: Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 3: Normalize (Token Replacement) - BEFORE removing punctuation\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' _URL_ ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' _EMAIL_ ', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\d+', ' _NUM_ ', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Step 4: Tokenize\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except Exception:\n",
    "        # Fallback to simple split if tokenization fails\n",
    "        tokens = text.split()\n",
    "    \n",
    "    # Step 5: Remove punctuation & stop words\n",
    "    clean_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in ['_URL_', '_EMAIL_', '_NUM_']:\n",
    "            clean_tokens.append(word)\n",
    "        elif word.isalpha() and word not in stop_words:\n",
    "            clean_tokens.append(word)\n",
    "    \n",
    "    # Step 6: Lemmatize\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in clean_tokens]\n",
    "    \n",
    "    # Step 7: Join to create cleaned text\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    # Check if cleaning resulted in empty text\n",
    "    if not cleaned_text.strip():\n",
    "        raise ValueError(\"Empty string after preprocessing - no meaningful content\")\n",
    "    \n",
    "    # Step 8: Vectorize for model prediction\n",
    "    vectorized_features = _VECTORIZER_CACHE.transform([cleaned_text])\n",
    "    \n",
    "    return cleaned_text, vectorized_features\n",
    "\n",
    "\n",
    "# Example usage function\n",
    "def predict_phishing(email_text, model_path='l1_nlp_model.pkl', vectorizer_path='l1_vectorizer.pkl'):\n",
    "    \"\"\"\n",
    "    Complete pipeline: preprocess email and predict if it's phishing.\n",
    "    \n",
    "    Args:\n",
    "        email_text (str): Raw email text\n",
    "        model_path (str): Path to saved model\n",
    "        vectorizer_path (str): Path to saved vectorizer\n",
    "        \n",
    "    Returns:\n",
    "        dict: Prediction results with cleaned text and probability scores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess\n",
    "        cleaned_text, features = preprocess_email_for_inference(email_text, vectorizer_path)\n",
    "        \n",
    "        # Load model and predict\n",
    "        model = joblib.load(model_path)\n",
    "        prediction = model.predict(features)[0]\n",
    "        probabilities = model.predict_proba(features)[0]\n",
    "        \n",
    "        # Get class labels in the correct order\n",
    "        class_labels = model.classes_\n",
    "        \n",
    "        # Create probability dictionary with correct mapping\n",
    "        prob_dict = {label: prob for label, prob in zip(class_labels, probabilities)}\n",
    "        \n",
    "        # Get the confidence for the predicted class\n",
    "        predicted_confidence = prob_dict[prediction]\n",
    "        \n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'is_phishing': prediction == 'Phishing Email',\n",
    "            'confidence': predicted_confidence,\n",
    "            'probabilities': prob_dict,\n",
    "            'cleaned_text': cleaned_text[:200] + '...' if len(cleaned_text) > 200 else cleaned_text\n",
    "        }\n",
    "        \n",
    "    except ValueError as e:\n",
    "        return {'error': str(e)}\n",
    "    except Exception as e:\n",
    "        return {'error': f\"Prediction failed: {str(e)}\"}\n",
    "\n",
    "\n",
    "# Test the function\n",
    "if __name__ == \"__main__\":\n",
    "    # Test email\n",
    "    test_email = \"\"\"\n",
    "    URGENT: Your account has been suspended!\n",
    "    Click here http://suspicious-link.com to verify your identity immediately.\n",
    "    We need your password and credit card information within 24 hours.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Testing preprocessing function...\\n\")\n",
    "    \n",
    "    try:\n",
    "        cleaned, features = preprocess_email_for_inference(test_email)\n",
    "        print(\"✓ Preprocessing successful!\")\n",
    "        print(f\"Cleaned text: {cleaned[:100]}...\")\n",
    "        print(f\"Feature shape: {features.shape}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Testing complete prediction pipeline...\\n\")\n",
    "        \n",
    "        result = predict_phishing(test_email)\n",
    "        if 'error' in result:\n",
    "            print(f\"✗ Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"✓ Prediction: {result['prediction']}\")\n",
    "            print(f\"  Is Phishing: {result['is_phishing']}\")\n",
    "            print(f\"  Confidence: {result['confidence']:.2%}\")\n",
    "            print(f\"  Probabilities:\")\n",
    "            for label, prob in result['probabilities'].items():\n",
    "                print(f\"    - {label}: {prob:.2%}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Test failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf3b811-94dd-4ea2-a95f-49b1d3dc7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK data...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8903643c-d828-4562-ae3b-144b5051dd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('account click _URL_ verify identity need password credit card information within _NUM_',\n",
       " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       " \twith 12 stored elements and shape (1, 5000)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae157d-78a9-4102-b39a-1d60d38ecc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
