{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa00fc16-d848-4c21-b419-cb4d53977e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: phishing\n",
      "Confidence: 76.59%\n",
      "Features: {'hop_count': 1, 'uses_url_shortener': 0, 'num_script_tags': 6, 'has_password_field': 0, 'num_external_links': 3, 'dom_depth': 11, 'final_url_tld_freq': 715}\n",
      "\n",
      "Probability Distribution:\n",
      "  Benign: 29.89%\n",
      "  Phishing: 70.11%\n",
      "\n",
      "Batch Predictions:\n",
      "https://www.google.com: phishing (78.16%)\n",
      "https://github.com: benign (52.09%)\n",
      "https://stackoverflow.com: benign (99.78%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import joblib\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "import whois\n",
    "from functools import lru_cache\n",
    "\n",
    "class URLPhishingDetector:\n",
    "    \"\"\"\n",
    "    A class to detect phishing URLs using Layer 2 and Layer 3 features.\n",
    "    Includes caching for model artifacts to improve performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path='l2_l3_xgboost_model.pkl', \n",
    "                 scaler_path='scaler.joblib',\n",
    "                 tld_map_path='l2_l3_tld_map.pkl'):\n",
    "        \"\"\"\n",
    "        Initialize the detector with paths to saved artifacts.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the trained model pickle file\n",
    "            scaler_path: Path to the fitted scaler pickle file\n",
    "            tld_map_path: Path to the TLD frequency map pickle file\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.scaler_path = scaler_path\n",
    "        self.tld_map_path = tld_map_path\n",
    "        \n",
    "        # Lazy loading - artifacts will be loaded on first use\n",
    "        self._model = None\n",
    "        self._scaler = None\n",
    "        self._tld_map = None\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"Lazy load and cache the model.\"\"\"\n",
    "        if self._model is None:\n",
    "            self._model = joblib.load(self.model_path)\n",
    "        return self._model\n",
    "    \n",
    "    @property\n",
    "    def scaler(self):\n",
    "        \"\"\"Lazy load and cache the scaler.\"\"\"\n",
    "        if self._scaler is None:\n",
    "            self._scaler = joblib.load(self.scaler_path)\n",
    "        return self._scaler\n",
    "    \n",
    "    @property\n",
    "    def tld_map(self):\n",
    "        \"\"\"Lazy load and cache the TLD frequency map.\"\"\"\n",
    "        if self._tld_map is None:\n",
    "            self._tld_map = joblib.load(self.tld_map_path)\n",
    "        return self._tld_map\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tld(url):\n",
    "        \"\"\"\n",
    "        Extract the top-level domain from a URL.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to parse\n",
    "            \n",
    "        Returns:\n",
    "            str: The TLD (e.g., 'com', 'org', 'edu')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            domain = urlparse(url).netloc\n",
    "            if not domain:\n",
    "                return 'none'\n",
    "            parts = domain.split('.')\n",
    "            if len(parts) > 1:\n",
    "                # Handle country-code TLDs like co.uk\n",
    "                return '.'.join(parts[-2:]) if len(parts[-1]) == 2 and len(parts[-2]) <= 3 else parts[-1]\n",
    "            return 'none'\n",
    "        except:\n",
    "            return 'error'\n",
    "    \n",
    "    def analyze_redirects(self, start_url):\n",
    "        \"\"\"\n",
    "        Follow redirect chain and extract Layer 2 features.\n",
    "        \n",
    "        Args:\n",
    "            start_url: The URL to analyze\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (final_url, l2_features_dict)\n",
    "        \"\"\"\n",
    "        l2_features = {\n",
    "            'hop_count': -1,\n",
    "            'uses_url_shortener': 0,\n",
    "            'final_url_tld': 'none'\n",
    "        }\n",
    "        \n",
    "        redirect_chain = []\n",
    "        current_url = str(start_url)\n",
    "        max_hops = 10\n",
    "        \n",
    "        # Check for URL shorteners\n",
    "        try:\n",
    "            if urlparse(current_url).netloc in ['bit.ly', 'tinyurl.com', 't.co', 'goo.gl', 'buff.ly']:\n",
    "                l2_features['uses_url_shortener'] = 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Follow redirects\n",
    "        try:\n",
    "            for i in range(max_hops):\n",
    "                response = requests.head(\n",
    "                    current_url, \n",
    "                    allow_redirects=False, \n",
    "                    timeout=5,\n",
    "                    headers={\n",
    "                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "                    }\n",
    "                )\n",
    "                redirect_chain.append(current_url)\n",
    "                \n",
    "                if 300 <= response.status_code < 400 and 'Location' in response.headers:\n",
    "                    next_url = response.headers['Location']\n",
    "                    if not urlparse(next_url).netloc:\n",
    "                        next_url = requests.compat.urljoin(current_url, next_url)\n",
    "                    current_url = next_url\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            l2_features['hop_count'] = len(redirect_chain) - 1\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Connection error - return None to indicate failure\n",
    "            return None, l2_features\n",
    "        \n",
    "        # Get final URL TLD\n",
    "        final_url = current_url\n",
    "        l2_features['final_url_tld'] = self.get_tld(final_url)\n",
    "        \n",
    "        return final_url, l2_features\n",
    "    \n",
    "    def analyze_html(self, url):\n",
    "        \"\"\"\n",
    "        Fetch and analyze HTML content to extract Layer 3 features.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to analyze\n",
    "            \n",
    "        Returns:\n",
    "            dict: Layer 3 features\n",
    "        \"\"\"\n",
    "        l3_features = {\n",
    "            'num_script_tags': -1,\n",
    "            'has_password_field': -1,\n",
    "            'num_external_links': -1,\n",
    "            'dom_depth': -1\n",
    "        }\n",
    "        \n",
    "        if not url:\n",
    "            return l3_features\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url, \n",
    "                timeout=10,\n",
    "                headers={\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Feature 1: Number of script tags\n",
    "            l3_features['num_script_tags'] = len(soup.find_all('script'))\n",
    "            \n",
    "            # Feature 2: Has password field\n",
    "            l3_features['has_password_field'] = 1 if soup.find('input', {'type': 'password'}) else 0\n",
    "            \n",
    "            # Feature 3: Number of external links\n",
    "            external_links = 0\n",
    "            current_domain = urlparse(url).netloc\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                link_domain = urlparse(link['href']).netloc\n",
    "                if link_domain and link_domain != current_domain:\n",
    "                    external_links += 1\n",
    "            l3_features['num_external_links'] = external_links\n",
    "            \n",
    "            # Feature 4: DOM depth\n",
    "            def get_depth(element, depth):\n",
    "                if not hasattr(element, 'children'):\n",
    "                    return depth\n",
    "                max_child_depth = depth\n",
    "                for child in element.children:\n",
    "                    if child.name:\n",
    "                        child_depth = get_depth(child, depth + 1)\n",
    "                        if child_depth > max_child_depth:\n",
    "                            max_child_depth = child_depth\n",
    "                return max_child_depth\n",
    "            \n",
    "            l3_features['dom_depth'] = get_depth(soup, 0)\n",
    "            \n",
    "        except requests.exceptions.RequestException:\n",
    "            pass  # Return default -1 values\n",
    "        except Exception:\n",
    "            pass  # Return default -1 values\n",
    "        \n",
    "        return l3_features\n",
    "    \n",
    "    def preprocess_url(self, url):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline for a single URL.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to analyze\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (features_dict, scaled_features_array) or raises exception\n",
    "            \n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the URL\n",
    "        \"\"\"\n",
    "        # Layer 2: Analyze redirects\n",
    "        final_url, l2_features = self.analyze_redirects(url)\n",
    "        \n",
    "        # Check if connection failed (hop_count == -1)\n",
    "        if l2_features['hop_count'] == -1:\n",
    "            raise ConnectionError(f\"Unable to connect to URL: {url}\")\n",
    "        \n",
    "        # Layer 3: Analyze HTML (only if L2 succeeded)\n",
    "        if final_url:\n",
    "            l3_features = self.analyze_html(final_url)\n",
    "        else:\n",
    "            l3_features = {\n",
    "                'num_script_tags': -1,\n",
    "                'has_password_field': -1,\n",
    "                'num_external_links': -1,\n",
    "                'dom_depth': -1\n",
    "            }\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = {\n",
    "            'hop_count': l2_features['hop_count'],\n",
    "            'uses_url_shortener': l2_features['uses_url_shortener'],\n",
    "            'num_script_tags': l3_features['num_script_tags'],\n",
    "            'has_password_field': l3_features['has_password_field'],\n",
    "            'num_external_links': l3_features['num_external_links'],\n",
    "            'dom_depth': l3_features['dom_depth']\n",
    "        }\n",
    "        \n",
    "        # Map TLD to frequency (1 if not in training data)\n",
    "        tld = l2_features['final_url_tld']\n",
    "        combined_features['final_url_tld_freq'] = self.tld_map.get(tld, 1)\n",
    "        \n",
    "        # Convert to DataFrame with correct column order\n",
    "        feature_columns = [\n",
    "            'hop_count', 'uses_url_shortener', 'num_script_tags',\n",
    "            'has_password_field', 'num_external_links', 'dom_depth',\n",
    "            'final_url_tld_freq'\n",
    "        ]\n",
    "        feature_values = [\n",
    "            combined_features['hop_count'],\n",
    "            combined_features['uses_url_shortener'],\n",
    "            combined_features['num_script_tags'],\n",
    "            combined_features['has_password_field'],\n",
    "            combined_features['num_external_links'],\n",
    "            combined_features['dom_depth'],\n",
    "            combined_features['final_url_tld_freq']\n",
    "        ]\n",
    "\n",
    "        \n",
    "        features_df = pd.DataFrame(combined_features, columns=feature_columns, index = [0])\n",
    "        \n",
    "        # Scale features\n",
    "        #scaled_features = self.scaler.transform(features_df)\n",
    "        #removing scaling because retrained model without scaling, trying to fix an issue, won't affect performacne for tree models\n",
    "        scaled_features = features_df\n",
    "        return combined_features, scaled_features\n",
    "    \n",
    "    def predict(self, url, return_probability=False):\n",
    "        \"\"\"\n",
    "        Predict whether a URL is phishing or benign.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to analyze\n",
    "            return_probability: If True, return probability scores instead of binary prediction\n",
    "            \n",
    "        Returns:\n",
    "            dict: Prediction result with the following keys:\n",
    "                - 'url': The analyzed URL\n",
    "                - 'prediction': 'phishing' or 'benign' (or probability if return_probability=True)\n",
    "                - 'confidence': Probability of the predicted class\n",
    "                - 'features': Dictionary of extracted features\n",
    "                \n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the URL\n",
    "        \"\"\"\n",
    "        # Preprocess the URL\n",
    "        features_dict, scaled_features = self.preprocess_url(url)\n",
    "        \n",
    "        # Make prediction\n",
    "        if return_probability:\n",
    "            # Return probability scores [benign_prob, phishing_prob]\n",
    "            probabilities = self.model.predict_proba(scaled_features)[0]\n",
    "            prediction = {\n",
    "                'benign': float(probabilities[0]),\n",
    "                'phishing': float(probabilities[1])\n",
    "            }\n",
    "            confidence = float(max(probabilities))\n",
    "        else:\n",
    "            # Return binary prediction (0 = benign, 1 = phishing)\n",
    "            pred = self.model.predict(scaled_features)[0]\n",
    "            probabilities = self.model.predict_proba(scaled_features)[0]\n",
    "            prediction = 'phishing' if pred == 1 else 'benign'\n",
    "            confidence = float(probabilities[pred])\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'features': features_dict\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage examples:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the detector (artifacts will be loaded on first use)\n",
    "    detector = URLPhishingDetector(\n",
    "        model_path='l2_l3_xgboost_model.pkl',\n",
    "        scaler_path='scaler.joblib',\n",
    "        tld_map_path='l2_l3_tld_map.pkl'\n",
    "    )\n",
    "    \n",
    "    # Example 1: Predict a single URL\n",
    "    try:\n",
    "        result = detector.predict(\"https://google.com\")\n",
    "        print(\"Prediction:\", result['prediction'])\n",
    "        print(\"Confidence:\", f\"{result['confidence']:.2%}\")\n",
    "        print(\"Features:\", result['features'])\n",
    "    except ConnectionError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Example 2: Get probability scores\n",
    "    try:\n",
    "        result = detector.predict(\"https://example.com\", return_probability=True)\n",
    "        print(\"\\nProbability Distribution:\")\n",
    "        print(f\"  Benign: {result['prediction']['benign']:.2%}\")\n",
    "        print(f\"  Phishing: {result['prediction']['phishing']:.2%}\")\n",
    "    except ConnectionError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Example 3: Batch processing (efficient with caching)\n",
    "    urls = [\n",
    "        \"https://www.google.com\",\n",
    "        \"https://github.com\",\n",
    "        \"https://stackoverflow.com\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nBatch Predictions:\")\n",
    "    for url in urls:\n",
    "        try:\n",
    "            result = detector.predict(url)\n",
    "            print(f\"{url}: {result['prediction']} ({result['confidence']:.2%})\")\n",
    "        except ConnectionError:\n",
    "            print(f\"{url}: Unable to connect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf2c387e-a892-47f0-bc07-bdb206835896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler expects this many features: 7\n",
      "\n",
      "Scaler feature names (if available):\n",
      "['hop_count' 'uses_url_shortener' 'num_script_tags' 'has_password_field'\n",
      " 'num_external_links' 'dom_depth' 'final_url_tld_freq']\n",
      "\n",
      "Model expects this many features: 7\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('l2_l3_xgboost_model.pkl')\n",
    "scaler = joblib.load('scaler.joblib')\n",
    "\n",
    "# Check what the scaler expects\n",
    "print(\"Scaler expects this many features:\", scaler.n_features_in_)\n",
    "print(\"\\nScaler feature names (if available):\")\n",
    "if hasattr(scaler, 'feature_names_in_'):\n",
    "    print(scaler.feature_names_in_)\n",
    "else:\n",
    "    print(\"Feature names not stored in scaler\")\n",
    "\n",
    "# Check model\n",
    "print(\"\\nModel expects this many features:\", model.n_features_in_)\n",
    "if hasattr(model, 'feature_names_in_'):\n",
    "    print(\"Model feature names:\")\n",
    "    print(model.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70c297b-dfc2-479e-a48b-653e6337a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for google.com: {'hop_count': 1, 'uses_url_shortener': 0, 'num_script_tags': 6, 'has_password_field': 0, 'num_external_links': 3, 'dom_depth': 11, 'final_url_tld_freq': 715}\n",
      "Scaled features: [[0.    0.    0.    0.    0.125 0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "detector = URLPhishingDetector(...)\n",
    "features, scaled = detector.preprocess_url(\"https://google.com\")\n",
    "print(\"Features for google.com:\", features)\n",
    "print(\"Scaled features:\", scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c793af3-7cdc-4a77-907c-c7f98cd0d507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected feature order: ['hop_count' 'uses_url_shortener' 'num_script_tags' 'has_password_field'\n",
      " 'num_external_links' 'dom_depth' 'final_url_tld_freq']\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected feature order:\", scaler.feature_names_in_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a64baf8c-c2a3-4b5a-a318-c3d3e3349a8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombined features dict:\u001b[39m\u001b[38;5;124m\"\u001b[39m, combined_features)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features_df\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame dtypes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features_df\u001b[38;5;241m.\u001b[39mdtypes)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Combined features dict:\", combined_features)\n",
    "print(\"DataFrame values:\", features_df.values)\n",
    "print(\"DataFrame dtypes:\", features_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf085fd-7c9c-48d5-aec5-e26b3b2e2c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
